# 决策树算法实现
Scikit-learn库中实现决策树算法的类
```python
# 分类树
sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, 
    min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None, 
    random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0,
    min_impurity_split=None, class_weight=None, presort=False)
# 回归树
sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None, 
    min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None, 
    random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0,
    min_impurity_split=None, presort=False)
```
## 参数解释
* criterion  
  + gini：CART算法，默认
  + entropy：ID3, C4.5
  + mse：默认
  + mae
    
* splitter=‘best'  
  + best：在特征的所有划分点中找出最优的划分点。默认的"best"适合样本量不大的时候
  * random：随机的在部分划分点中找局部最优的划分点。如果样本数据量非常大，此时决策树构建推荐"random"   
* max_depth=None
  + 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度
  + 一般来说，数据少或者特征少的时候可以不管这个值
  + 如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。
  + 常用的可以取值10-100之间。
* max_features=None

  + None：划分时考虑的最大特征数，默认是"None",意味着划分时考虑所有的特征数
  + 'log2'：划分时最多考虑log2N个特征
  + 'sqrt'、'auto'：划分时最多考虑sqrt(N)个特征
  + int：代表考虑的特征绝对数。
  + float：代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了
如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

* min_samples_split=2

  + 这个值限制了子树继续划分的条件   
  + 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2  
  + 如果样本量不大，不需要管这个值  
  + 如果样本量数量级非常大，则推荐增大这个值  
  + 参考：有大概10万样本，建立决策树时，我选择了min_samples_split=10。  
    
* min_samples_leaf=1

    + 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝
    + 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。
    + 如果样本量不大，不需要管这个值
    + 如果样本量数量级非常大，则推荐增大这个值。
    + 之前的10万样本项目使用min_samples_leaf的值为5
* min_weight_fraction_leaf=0

    + 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。
    + 默认是0，就是不考虑权重问题
    + 一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。
* max_leaf_nodes=None

    + 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。
    + 如果加了限制，算法会建立在最大叶子节点数内最优的决策树。
    + 如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。
* class_weight

    + 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别
    + balanced：算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的"None"
* min_impurity_split

    + 节点划分最小不纯度
    + 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。
* presort

    + 数据是否预排序，这个值是布尔值，默认是False不排序
    + 一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快
    + 如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。


