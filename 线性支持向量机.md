
# 支持向量机

支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短短的二十多年，但是自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现SVM说是排第一估计是没有什么异议的。
SVM是一个二元分类算法，线性分类和非线性分类都支持。经过演进，现在也可以支持多元分类，同时经过扩展，也能应用于回归问题。本系列文章就对SVM的原理做一个总结。本篇的重点是SVM用于线性分类时模型和损失函数优化的一个总结。

## 感知机模型
在感知机原理小结中，我们讲到了感知机的分类原理，感知机的模型就是尝试找到一条直线，能够把二元数据隔离开。
放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。
对于这个分离的超平面，我们定义为$w^Tx+b=0$，如下图。在超平面$wTx+b=0$上方的我们定义为$y=1$,在超平面$wTx+b=0$下方的我们定义为$y=−1$。
可以看出满足这个条件的超平面并不止一个。
那么我们可能会尝试思考，这么多的可以分类的超平面，哪个是最好的呢？或者说哪个是泛化能力最强的呢?

## 间隔与支持向量
给定训练集，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。直观来看，位于两类训练样本“正中间”的划分超平面效果最好，即中间最粗的那条。

![](http://o73nd1ra4.bkt.clouddn.com/ML7-1.png)

在样本空间中，划分超平面可通过如下线性方程来描述：$w^Tx+b=0$, $w$为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。样本空间中任意点x到超平面$（w,b）$的距离可写为：$$r=|w^T x+b|/‖w‖ $$。
假设超平面$（w,b）$能将训练样本正确分类，令
$${■(w^T x_i+b≥+1&y_i=+1@w^T x_i+b≤-1&y_i=-1)┤$$
如下图6.2所示，距离超平面最近的这几个训练样本点使式（6.3）的等号成立，它们被称为“支持向量”（support vector），两个异类支持向量到超平面的距离之和称为“间隔”（margin），公式为：


欲找到具有“最大间隔”（maximum margin）的划分超平面，也就是要找到能满足式（6.3）中约束的参数w和b，使得间隔最大，即

最大化间隔等价于最小化||w||，于是，式（6.5）可重写为

上式为支持向量机（Support Vector Machine，SVM）的基本型。



<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
