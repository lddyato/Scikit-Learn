支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短短的二十多年，但是自一诞生便由于它良好的分类性能席卷了机器学习领域，并牢牢压制了神经网络领域好多年。如果不考虑集成学习的算法，不考虑特定的训练数据集，在分类算法中的表现SVM说是排第一估计是没有什么异议的。
SVM是一个二元分类算法，线性分类和非线性分类都支持。经过演进，现在也可以支持多元分类，同时经过扩展，也能应用于回归问题。本系列文章就对SVM的原理做一个总结。本篇的重点是SVM用于线性分类时模型和损失函数优化的一个总结。

## 感知机模型
在感知机原理小结中，我们讲到了感知机的分类原理，感知机的模型就是尝试找到一条直线，能够把二元数据隔离开。
放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。
对于这个分离的超平面，我们定义为$w^Tx+b=0$，如下图。在超平面$wTx+b=0$上方的我们定义为$y=1$,在超平面$wTx+b=0$下方的我们定义为$y=−1$。
可以看出满足这个条件的超平面并不止一个。
那么我们可能会尝试思考，这么多的可以分类的超平面，哪个是最好的呢？或者说哪个是泛化能力最强的呢?

<img style="display: block; margin-left: auto; margin-right: auto" src="http://images2015.cnblogs.com/blog/1042406/201611/1042406-20161124135616081-623185925.jpg" alt="" width="422" height="294">

接着我们看感知机模型的损失函数优化，它的思想是让所有误分类的点(定义为M)到超平面的距离和最小，即最小化下式：$$\sum\limits\_{x\_i \in M}- y^{(i)}(w^Tx^{(i)} +b)\big / ||w||\_2$$

当\(w和b\)成比例的增加，比如,当分子的\(w和b\)扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，固定分母$||w||\_2 = 1$,即最终感知机模型的损失函数为：$$\sum\limits\_{x\_i \in M}- y^{(i)}(w^Tx^{(i)} +b)$$
如果我们不是固定分母，改为固定分子，作为分类模型有没有改进呢？


## 间隔与支持向量
给定训练集，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。直观来看，位于两类训练样本“正中间”的划分超平面效果最好，即中间最粗的那条。

![](http://o73nd1ra4.bkt.clouddn.com/ML7-1.png)

在分离超平面固定为$w^Tx + b = 0$的时候，$|w^Tx + b&nbsp;|$表示点x到超平面的距离。通过观察$w^Tx + b $和y是否同号，我们判断分类是否正确，这些知识我们在感知机模型里都有讲到。这里我们引入函数间隔的概念，定义函数间隔$\gamma^{'}$为：$$\gamma^{'} = y(w^Tx + b)$$。可以看到，它就是感知机模型里面的误分类点到超平面距离的分子。对于训练集中m个样本点对应的m个函数间隔的最小值，就是整个训练集的函数间隔。
函数间隔并不能正常反应点到超平面的距离，在感知机模型里我们也提到，当分子成比例的增长时，分母也是成倍增长。为了统一度量，我们需要对法向量$w$加上约束条件，这样我们就得到了几何间隔$\gamma$,定义为：$$\gamma = \frac{y(w^Tx + b)}{||w||\_2} = &nbsp;\frac{\gamma^{'}}{||w||\_2}$$。几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。


欲找到具有“最大间隔”（maximum margin）的划分超平面，也就是要找到能满足式（6.3）中约束的参数w和b，使得间隔最大，即

最大化间隔等价于最小化||w||，于是，式（6.5）可重写为

上式为支持向量机（Support Vector Machine，SVM）的基本型。



<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>支持向量机原理(四)SMO算法原理 - 刘建平Pinard - 博客园</title>
<link type="text/css" rel="stylesheet" href="/bundles/blog-common.css?v=-wBWg2jMfLNV0-ScpDNxGkoH_gCbdW1yTVJLHzPL7HE1"/>
<link id="MainCss" type="text/css" rel="stylesheet" href="/skins/BlackLowKey/bundle-BlackLowKey.css?v=B7-m2NxlKbJ6xN_kGRXgbaLp9_4SHPK1jC_DQ8UbhqA1"/>
<link type="text/css" rel="stylesheet" href="/blog/customcss/311024.css?v=QLzUGrIlosL9ay3UmSubmXjU7i0%3d"/>
<link id="mobile-style" media="only screen and (max-width: 768px)" type="text/css" rel="stylesheet" href="/skins/BlackLowKey/bundle-BlackLowKey-mobile.css?v=hM_4ic4i3O5wKXUjikUYwUGh6o9_4est1wSubdkIKhE1"/>
<link title="RSS" type="application/rss+xml" rel="alternate" href="http://www.cnblogs.com/pinard/rss"/>
<link title="RSD" type="application/rsd+xml" rel="EditURI" href="http://www.cnblogs.com/pinard/rsd.xml"/>
<link type="application/wlwmanifest+xml" rel="wlwmanifest" href="http://www.cnblogs.com/pinard/wlwmanifest.xml"/>
<script src="//common.cnblogs.com/script/jquery.js" type="text/javascript"></script>  
<script type="text/javascript">var currentBlogApp = 'pinard', cb_enable_mathjax=true;var isLogined=false;</script>
<script src="/bundles/blog-common.js?v=-sYWYg_JMmWLy2BUXo9NpUbvg7jV5TLtD6QKvvKrgzg1" type="text/javascript"></script>
</head>
<body>
<a name="top"></a>

<!--done-->
<div id="home">
<div id="header">
	<div id="blogTitle">
	<a id="lnkBlogLogo" href="http://www.cnblogs.com/pinard/"><img id="blogLogo" src="/Skins/custom/images/logo.gif" alt="返回主页" /></a>			
		
<!--done-->
<h1><a id="Header1_HeaderTitle" class="headermaintitle" href="http://www.cnblogs.com/pinard/">刘建平Pinard</a></h1>
<h2>十年码农，对数学统计学，数据挖掘，机器学习，大数据平台，大数据平台应用开发，大数据可视化感兴趣。</h2>



		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li><a id="blog_nav_sitehome" class="menu" href="http://www.cnblogs.com/">博客园</a></li>
<li><a id="blog_nav_myhome" class="menu" href="http://www.cnblogs.com/pinard/">首页</a></li>
<li><a id="blog_nav_newpost" class="menu" rel="nofollow" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">新随笔</a></li>
<li><a id="blog_nav_contact" class="menu" rel="nofollow" href="https://msg.cnblogs.com/send/%E5%88%98%E5%BB%BA%E5%B9%B3Pinard">联系</a></li>
<li><a id="blog_nav_rss" class="menu" href="http://www.cnblogs.com/pinard/rss">订阅</a>
<!--<a id="blog_nav_rss_image" class="aHeaderXML" href="http://www.cnblogs.com/pinard/rss"><img src="//www.cnblogs.com/images/xml.gif" alt="订阅" /></a>--></li>
<li><a id="blog_nav_admin" class="menu" rel="nofollow" href="https://i.cnblogs.com/">管理</a></li>
</ul>
		<div class="blogStats">
			
			<div id="blog_stats">
<span id="stats_post_count">随笔 - 101&nbsp; </span>
<span id="stats_article_count">文章 - 0&nbsp; </span>
<span id="stats-comment_count">评论 - 458</span>
</div>
			
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->

<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		
<div id="post_detail">
<!--done-->
<div id="topics">
	<div class = "post">
		<h1 class = "postTitle">
			<a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6111471.html">支持向量机原理(四)SMO算法原理</a>
		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a id="homepage1_HomePageDays_DaysList_ctl01_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6097604.html">支持向量机原理(一) 线性支持向量机</a></p>
<p>　　　　<a id="homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6100722.html">支持向量机原理(二) 线性支持向量机的软间隔最大化模型</a></p>
<p>　　　　<a id="homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6103615.html">支持向量机原理(三)线性不可分支持向量机与核函数</a></p>
<p>　　　　<a id="homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6111471.html">支持向量机原理(四)SMO算法原理</a></p>
<p>　　　　<a id="post_title_link_6113120" href="http://www.cnblogs.com/pinard/p/6113120.html">支持向量机原理(五)线性支持回归</a></p>
<p>&nbsp;</p>
<p>　　在SVM的前三篇里，我们优化的目标函数最终都是一个关于$\alpha$向量的函数。而怎么极小化这个函数，求出对应的$\alpha$向量，进而求出分离超平面我们没有讲。本篇就对优化这个关于$\alpha$向量的函数的SMO算法做一个总结。</p>
<h1>1. 回顾SVM优化目标函数</h1>
<p>　　　　我们首先回顾下我们的优化目标函数：$$ \underbrace{ min }_{\alpha} &nbsp;\frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum\limits_{i=1}^{m}\alpha_i $$ $$ s.t. \; \sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$0 \leq \alpha_i \leq C$$</p>
<p>　　　　我们的解要满足的KKT条件的对偶互补条件为：$$\alpha_{i}^{*}(y_i(w^{*} \bullet \phi(x_i) + b^{*}) - 1) = 0$$</p>
<p>　　　　根据这个KKT条件的对偶互补条件，我们有：$$\alpha_{i}^{*} = 0 \Rightarrow y_i(w^{*} \bullet \phi(x_i) + b)&nbsp;\geq&nbsp;1 $$ $$ 0 \leq \alpha_{i}^{*}&nbsp;\leq C &nbsp;\Rightarrow y_i(w^{*} \bullet \phi(x_i) + b)&nbsp;=&nbsp;1 $$ $$\alpha_{i}^{*}=&nbsp;C \Rightarrow y_i(w^{*} \bullet \phi(x_i) + b)&nbsp;\leq 1$$</p>
<p>　　　　 由于$w^{*} = \sum\limits_{j=1}^{m}\alpha_j^{*}y_j\phi(x_j)$,我们令$g(x) = w^{*} \bullet \phi(x) + b =\sum\limits_{j=1}^{m}\alpha_j^{*}y_jK(x, x_j)+ b^{*}$，则有： $$\alpha_{i}^{*} = 0 \Rightarrow y_ig(x_i)&nbsp;\geq&nbsp;1 $$ $$ 0 \leq \alpha_{i}^{*}&nbsp;\leq C &nbsp;\Rightarrow y_ig(x_i)&nbsp;&nbsp;=&nbsp;1 $$ $$\alpha_{i}^{*}=&nbsp;C \Rightarrow y_ig(x_i)&nbsp;&nbsp;\leq 1$$</p>
<h1>2. SMO算法的基本思想</h1>
<p>　　　　上面这个优化式子比较复杂，里面有m个变量组成的向量$\alpha$需要在目标函数极小化的时候求出。直接优化时很难的。SMO算法则采用了一种启发式的方法。它每次只优化两个变量，将其他的变量都视为常数。由于$\sum\limits_{i=1}^{m}\alpha_iy_i = 0$.假如将$\alpha_3, \alpha_4, ..., \alpha_m$　固定，那么$\alpha_1, \alpha_2$之间的关系也确定了。这样SMO算法将一个复杂的优化算法转化为一个比较简单的两变量优化问题。</p>
<p>　　　　为了后面表示方便，我们定义$K_{ij} = \phi(x_i) \bullet \phi(x_j)$</p>
<p>　　　　由于$\alpha_3, \alpha_4, ..., \alpha_m$都成了常量，所有的常量我们都从目标函数去除，这样我们上一节的目标优化函数变成下式：$$\;\underbrace{ min }_{\alpha_1, \alpha_1} \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 +y_1y_2K_{12}\alpha_1 \alpha_2 -(\alpha_1 + \alpha_2)&nbsp;+y_1\alpha_1\sum\limits_{i=3}^{m}y_i\alpha_iK_{i1} + y_2\alpha_2\sum\limits_{i=3}^{m}y_i\alpha_iK_{i2}$$ $$s.t. \;\;\alpha_1y_1 + &nbsp;\alpha_2y_2 = -\sum\limits_{i=3}^{m}y_i\alpha_i = \varsigma&nbsp;$$ $$0 \leq \alpha_i \leq C \;\; i =1,2$$</p>
<h1>3. SMO算法目标函数的优化</h1>
<p>　　　　为了求解上面含有这两个变量的目标优化问题，我们首先分析约束条件，所有的$\alpha_1, \alpha_2$都要满足约束条件，然后在约束条件下求最小。</p>
<p>　　　　根据上面的约束条件$\alpha_1y_1 + &nbsp;\alpha_2y_2&nbsp; = \varsigma\;\;0 \leq \alpha_i \leq C \;\; i =1,2$，又由于$y_1,y_2$均只能取值1或者-1, 这样$\alpha_1, \alpha_2$在[0,C]和[0,C]形成的盒子里面，并且两者的关系直线的斜率只能为1或者-1，也就是说$\alpha_1, \alpha_2$的关系直线平行于[0,C]和[0,C]形成的盒子的对角线，如下图所示：</p>
<p><img style="margin-right: auto; margin-left: auto; display: block" src="http://images2015.cnblogs.com/blog/1042406/201611/1042406-20161128221540099-1580490663.png" alt=""></p>
<p>&nbsp;　　　　由于$\alpha_1, \alpha_2$的关系被限制在盒子里的一条线段上，所以两变量的优化问题实际上仅仅是一个变量的优化问题。不妨我们假设最终是$\alpha_2$的优化问题。由于我们采用的是启发式的迭代法，假设我们上一轮迭代得到的解是$\alpha_1^{old}, \alpha_2^{old}$，假设沿着约束方向$\alpha_2$未经剪辑的解是$\alpha_2^{new,unc}$.本轮迭代完成后的解为$\alpha_1^{new}, \alpha_2^{new}$</p>
<p>　　　　由于$\alpha_2^{new}$必须满足上图中的线段约束。假设L和H分别是上图中$\alpha_2^{new}$所在的线段的边界。那么很显然我们有：$$L \leq \alpha_2^{new} \leq H $$</p>
<p>　　　　而对于L和H，我们也有限制条件如果是上面左图中的情况，则$$L = max(0, \alpha_2^{old}-\alpha_1^{old}) \;\;\;H = min(C, C+\alpha_2^{old}-\alpha_1^{old})$$</p>
<p>　　　　如果是上面右图中的情况，我们有：$$L = max(0, \alpha_2^{old}+\alpha_1^{old}-C) \;\;\; H = min(C, \alpha_2^{old}+\alpha_1^{old})$$</p>
<p>&nbsp;　　　　也就是说，假如我们通过求导得到的$\alpha_2^{new,unc}$，则最终的$\alpha_2^{new}$应该为：</p>
<p>$$\alpha_2^{new}=<br>\begin{cases}<br>H&amp; {L \leq \alpha_2^{new,unc}&nbsp;&gt; H}\\<br>\alpha_2^{new,unc}&amp; {L \leq \alpha_2^{new,unc}&nbsp;\leq H}\\<br>L&amp; {\alpha_2^{new,unc} &lt; L}<br>\end{cases}$$　　　</p>
<p>　　　　那么如何求出$\alpha_2^{new,unc}$呢？很简单，我们只需要将目标函数对$\alpha_2$求偏导数即可。</p>
<p>　　　　首先我们整理下我们的目标函数。</p>
<p>　　　　为了简化叙述，我们令$$E_i = g(x_i)-y_i = \sum\limits_{j=1}^{m}\alpha_j^{*}y_jK(x_i, x_j)+ b - y_i$$，</p>
<p>　　　　其中$g(x)$就是我们在第一节里面的提到的$$g(x) = w^{*} \bullet \phi(x) + b =\sum\limits_{j=1}^{m}\alpha_j^{*}y_jK(x, x_j)+ b^{*}$$</p>
<p>　　　　我们令$$v_i = \sum\limits_{i=3}^{m}y_j\alpha_jK(x_i,x_j) = g(x_i) - &nbsp;\sum\limits_{i=1}^{2}y_j\alpha_jK(x_i,x_j) -b &nbsp;$$</p>
<p>　　　　这样我们的优化目标函数进一步简化为：$$W(\alpha_1,\alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 +y_1y_2K_{12}\alpha_1 \alpha_2 -(\alpha_1 + \alpha_2)&nbsp;+y_1\alpha_1v_1 + &nbsp;y_2\alpha_2v_2$$</p>
<p>　　　　由于$\alpha_1y_1 + &nbsp;\alpha_2y_2 = &nbsp;\varsigma&nbsp;$，并且$y_i^2 = 1$，可以得到$\alpha_1用 \alpha_2$表达的式子为：$$\alpha_1 = y_1(\varsigma&nbsp; - \alpha_2y_2)$$</p>
<p>　　　　将上式带入我们的目标优化函数，就可以消除$\alpha_1$,得到仅仅包含$\alpha_2$的式子。$$W(\alpha_2) = \frac{1}{2}K_{11}(\varsigma&nbsp; - \alpha_2y_2)^2 + \frac{1}{2}K_{22}\alpha_2^2 +y_2K_{12}(\varsigma&nbsp;- \alpha_2y_2) \alpha_2 -(\alpha_1 + \alpha_2)&nbsp;+(\varsigma&nbsp; - \alpha_2y_2)v_1 + &nbsp;y_2\alpha_2v_2$$</p>
<p>　　　　忙了半天，我们终于可以开始求$\alpha_2^{new,unc}$了，现在我们开始通过求偏导数来得到$\alpha_2^{new,unc}$。</p>
<p>$$\frac{\partial W}{\partial \alpha_2} = K_{11}\alpha_2 + &nbsp;K_{22}\alpha_2 -2K_{12}\alpha_2 - &nbsp;K_{11}\varsigma&nbsp;y_2 + K_{12}\varsigma&nbsp;y_2 +y_1y_2 -1 -v_1y_2 +y_2v_2 = 0$$</p>
<p>　　　　整理上式有：$$(K_{11} +K_{22}-2K_{12})\alpha_2 = y_2(y_2-y_1 + \varsigma&nbsp; K_{11} -&nbsp;\varsigma&nbsp; K_{12} + v_1 - v_2)$$</p>
<p>$$ =&nbsp;y_2(y_2-y_1 + \varsigma&nbsp; K_{11}&nbsp;- \varsigma&nbsp; K_{12} + (g(x_1) - &nbsp;\sum\limits_{j=1}^{2}y_j\alpha_jK_{1j} -b ) -(g(x_2) - &nbsp;\sum\limits_{j=1}^{2}y_j\alpha_jK_{2j} -b))$$</p>
<p>　　　　将$&nbsp;\varsigma&nbsp; = \alpha_1y_1 + &nbsp;\alpha_2y_2 $带入上式，我们有：</p>
<p>$$(K_{11} +K_{22}-2K_{12})\alpha_2^{new,unc} = y_2((K_{11} +K_{22}-2K_{12})\alpha_2^{old}y_2 +y_2-y_1 +g(x_1) - g(x_2))$$</p>
<p>$$\;\;\;\; = (K_{11} +K_{22}-2K_{12})&nbsp;\alpha_2^{old} + y2(E_1-E_2)$$</p>
<p>　　　　我们终于得到了$\alpha_2^{new,unc}$的表达式：$$\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y2(E_1-E_2)}{K_{11} +K_{22}-2K_{12})}$$</p>
<p>　　　　利用上面讲到的$\alpha_2^{new,unc}$和$\alpha_2^{new}$的关系式，我们就可以得到我们新的$\alpha_2^{new}$了。利用$\alpha_2^{new}$和$\alpha_1^{new}$的线性关系，我们也可以得到新的$\alpha_1^{new}$。</p>
<h1>4. SMO算法两个变量的选择</h1>
<p>　　　　SMO算法需要选择合适的两个变量做迭代，其余的变量做常量来进行优化，那么怎么选择这两个变量呢？</p>
<h2>4.1 第一个变量的选择</h2>
<p>　　　　SMO算法称选择第一个变量为外层循环，这个变量需要选择在训练集中违反KKT条件最严重的样本点。对于每个样本点，要满足的KKT条件我们在第一节已经讲到了：&nbsp;$$\alpha_{i}^{*} = 0 \Rightarrow y_ig(x_i)&nbsp;\geq&nbsp;1 $$ $$ 0 \leq \alpha_{i}^{*}&nbsp;\leq C &nbsp;\Rightarrow y_ig(x_i)&nbsp;&nbsp;=1 $$ $$\alpha_{i}^{*}=&nbsp;C \Rightarrow y_ig(x_i)&nbsp;&nbsp;\leq 1$$</p>
<p>　　　　一般来说，我们首先选择违反$0 \leq \alpha_{i}^{*}&nbsp;\leq C &nbsp;\Rightarrow y_ig(x_i)&nbsp;&nbsp;=1 $这个条件的点。如果这些支持向量都满足KKT条件，再选择违反$\alpha_{i}^{*} = 0 \Rightarrow y_ig(x_i)&nbsp;\geq&nbsp;1 $&nbsp;和 $\alpha_{i}^{*}=&nbsp;C \Rightarrow y_ig(x_i)&nbsp;&nbsp;\leq 1$的点。</p>
<h2>4.2 第二个变量的选择</h2>
<p>&nbsp;　　　　SMO算法称选择第二一个变量为内层循环，假设我们在外层循环已经找到了$\alpha_1$, 第二个变量$\alpha_2$的选择标准是让$|E1-E2|$有足够大的变化。由于$\alpha_1$定了的时候,$E_1$也确定了，所以要想$|E1-E2|$最大，只需要在$E_1$为正时，选择最小的$E_i$作为$E_2$， 在$E_1$为负时，选择最大的$E_i$作为$E_2$，可以将所有的$E_i$保存下来加快迭代。</p>
<p>　　　　如果内存循环找到的点不能让目标函数有足够的下降， 可以采用遍历支持向量点来做$\alpha_2$,直到目标函数有足够的下降， 如果所有的支持向量做$\alpha_2$都不能让目标函数有足够的下降，可以跳出循环，重新选择$\alpha_1$　</p>
<h2>4.3 计算阈值b和差值$E_i$　</h2>
<p>　　　　在每次完成两个变量的优化之后，需要重新计算阈值b。当$0 \leq \alpha_{1}^{new}&nbsp;\leq C$时，我们有 $$y_1 - \sum\limits_{i=1}^{m}\alpha_iy_iK_{i1} -b_1 = 0 $$</p>
<p>　　　　于是新的$b_1^{new}$为：$$b_1^{new} = y_1 - \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1}&nbsp;- \alpha_{1}^{new}y_1K_{11} - \alpha_{2}^{new}y_2K_{21}&nbsp;$$</p>
<p>　　　　计算出$E_1$为：$$E_1 = g(x_1) - y_1 = \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1} + \alpha_{1}^{old}y_1K_{11} + \alpha_{2}^{old}y_2K_{21} + b^{old} -y_1$$</p>
<p>　　　　可以看到上两式都有$y_1 - \sum\limits_{i=3}^{m}\alpha_iy_iK_{i1}$，因此可以将$b_1^{new}$用$E_1$表示为：$$b_1^{new} = -E_1 -y_1K_{11}(\alpha_{1}^{new} - \alpha_{1}^{old}) -y_2K_{21}(\alpha_{2}^{new} - \alpha_{2}^{old}) + b^{old}$$</p>
<p>　　　　同样的，如果$0 \leq \alpha_{2}^{new}&nbsp;\leq C$, 那么有：$$b_2^{new} = -E_2 -y_1K_{12}(\alpha_{1}^{new} - \alpha_{1}^{old}) -y_2K_{22}(\alpha_{2}^{new} - \alpha_{2}^{old}) + b^{old}$$</p>
<p>　　　　最终的$b^{new}$为：$$b^{new} = \frac{b_1^{new} + b_2^{new}}{2}$$</p>
<p>　　　　得到了$b^{new}$我们需要更新$E_i$:$$E_i = \sum\limits_{S}y_j\alpha_jK(x_i,x_j) + b^{new} -y_i&nbsp;$$</p>
<p>　　　　其中，S是所有支持向量$x_j$的集合。</p>
<p>　　　　好了，SMO算法基本讲完了，我们来归纳下SMO算法。</p>
<h1>5. SMO算法总结</h1>
<p>　　　　输入是m个样本${(x_1,y_1), (x_2,y_2), ..., (x_m,y_m),}$,其中x为n维特征向量。y为二元输出，值为1，或者-1.精度e。</p>
<p>　　　　输出是近似解$\alpha$</p>
<p>　　　　1)取初值$\alpha^{0} = 0, k =0$</p>
<p>　　　　2)按照4.1节的方法选择$\alpha_1^k$,接着按照4.2节的方法选择$\alpha_2^k$，求出新的$\alpha_2^{new,unc}$。$$\alpha_2^{new,unc} = \alpha_2^{k} + \frac{y_2(E_1-E_2)}{K_{11} +K_{22}-2K_{12})}$$</p>
<p>　　　　3)按照下式求出$\alpha_2^{k+1}$ </p>
<p>$$\alpha_2^{k+1}=<br>\begin{cases}<br>H&amp; {L \leq \alpha_2^{new,unc}&nbsp;&gt; H}\\<br>\alpha_2^{new,unc}&amp; {L \leq \alpha_2^{new,unc}&nbsp;\leq H}\\<br>L&amp; {\alpha_2^{new,unc} &lt; L}<br>\end{cases}$$</p>
<p>　　　　4)利用$\alpha_2^{k+1}$和$\alpha_1^{k+1}$的关系求出$\alpha_1^{k+1}$</p>
<p>　　　　5)按照4.3节的方法计算$b^{k+1}$和$E_i$</p>
<p>　　　　6）在精度e范围内检查是否满足如下的终止条件：$$\sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$0 \leq \alpha_i \leq C, i =1,2...m$$ $$\alpha_{i}^{k+1} = 0 \Rightarrow y_ig(x_i)&nbsp;\geq&nbsp;1 $$ $$ 0 \leq \alpha_{i}^{k+1}&nbsp;\leq C &nbsp;\Rightarrow y_ig(x_i)&nbsp;&nbsp;=&nbsp;1 $$ $$\alpha_{i}^{k+1}=&nbsp;C \Rightarrow y_ig(x_i)&nbsp;&nbsp;\leq 1$$</p>
<p>　　　　7)如果满足则结束，返回$\alpha^{k+1}$,否则转到步骤2）。</p>
<p>&nbsp;</p>
<p>　　　　SMO算法终于写完了，这块在以前学的时候是非常痛苦的，不过弄明白就豁然开朗了。希望大家也是一样。写完这一篇， SVM系列就只剩下支持向量回归了，胜利在望！</p>
<p>（欢迎转载，转载请注明出处。欢迎沟通交流： pinard.liu@ericsson.com）&nbsp;</p></div><div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
<div id="BlogPostCategory"></div>
<div id="EntryTag"></div>
<div id="blog_post_info">
</div>
<div class="clear"></div>
<div id="post_next_prev"></div>
</div>


		</div>
		<div class = "postDesc">posted @ <span id="post-date">2016-11-29 00:11</span> <a href='http://www.cnblogs.com/pinard/'>刘建平Pinard</a> 阅读(<span id="post_view_count">...</span>) 评论(<span id="post_comment_count">...</span>)  <a href ="https://i.cnblogs.com/EditPosts.aspx?postid=6111471" rel="nofollow">编辑</a> <a href="#" onclick="AddToWz(6111471);return false;">收藏</a></div>
	</div>
	<script type="text/javascript">var allowComments=true,cb_blogId=311024,cb_entryId=6111471,cb_blogApp=currentBlogApp,cb_blogUserGuid='7d95b75d-b891-e611-845c-ac853d9f53ac',cb_entryCreatedDate='2016/11/29 0:11:00';loadViewCount(cb_entryId);</script>
	
</div><!--end: topics 文章、评论容器-->
</div><a name="!comments"></a><div id="blog-comments-placeholder"></div><script type="text/javascript">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>
<div id='comment_form' class='commentform'>
<a name='commentform'></a>
<div id='divCommentShow'></div>
<div id='comment_nav'><span id='span_refresh_tips'></span><a href='javascript:void(0);' onclick='return RefreshCommentList();' id='lnk_RefreshComments' runat='server' clientidmode='Static'>刷新评论</a><a href='#' onclick='return RefreshPage();'>刷新页面</a><a href='#top'>返回顶部</a></div>
<div id='comment_form_container'></div>
<div class='ad_text_commentbox' id='ad_text_under_commentbox'></div>
<div id='ad_t2'></div>
<div id='opt_under_post'></div>
<div id='cnblogs_c1' class='c_ad_block'></div>
<div id='under_post_news'></div>
<div id='cnblogs_c2' class='c_ad_block'></div>
<div id='under_post_kb'></div>
<div id='HistoryToday' class='c_ad_block'></div>
<script type='text/javascript'>
    fixPostBody();
    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
    deliverAdT2();
    deliverAdC1();
    deliverAdC2();    
    loadNewsAndKb();
    loadBlogSignature();
    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate);
    loadOptUnderPost();
    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   
</script>
</div>


	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->

	<div id="sideBar">
		<div id="sideBarMain">
			
<!--done-->
<div class="newsItem">
<h3 class="catListTitle">公告</h3>
	<div id="blog-news"></div><script type="text/javascript">loadBlogNews();</script>
</div>

			<div id="blog-calendar" style="display:none"></div><script type="text/javascript">loadBlogDefaultCalendar();</script>
			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn"></div><script type="text/javascript">loadBlogSideColumn();</script>
			</div>
			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		
<!--done-->
Copyright &copy;2017 刘建平Pinard
	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->
</body>
</html>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
